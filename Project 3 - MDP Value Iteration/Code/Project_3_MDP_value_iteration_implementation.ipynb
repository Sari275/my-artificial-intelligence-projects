{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Classes and Definitions"
      ],
      "metadata": {
        "id": "Gk8NX6lOG5E3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "KW8XkG84zAks"
      },
      "outputs": [],
      "source": [
        "class OverheatingCarMDP:\n",
        "    state_set = [\"cool\", \"warm\", \"overheated\"]\n",
        "\n",
        "    def reward(self, state, action, state_prime):\n",
        "        if state_prime == \"overheated\":\n",
        "            return -10\n",
        "        elif action == \"slow\":\n",
        "            return 1\n",
        "        elif action == \"fast\":\n",
        "            return 2\n",
        "        else:\n",
        "            return 0\n",
        "\n",
        "    def transition_prob(self, state, action, state_prime):\n",
        "        probs = {\n",
        "            (\"cool\", \"slow\", \"cool\"): 1,\n",
        "            (\"cool\", \"fast\", \"cool\"): 0.5,\n",
        "            (\"cool\", \"fast\", \"warm\"): 0.5,\n",
        "            (\"warm\", \"slow\", \"cool\"): 0.5,\n",
        "            (\"warm\", \"slow\", \"warm\"): 0.5,\n",
        "            (\"warm\", \"fast\", \"overheated\"): 1\n",
        "        }\n",
        "        return probs.get((state, action, state_prime), 0)\n",
        "\n",
        "    def successor_states(self, state, action):\n",
        "        successors = {\n",
        "            (\"cool\", \"slow\"): [\"cool\"],\n",
        "            (\"cool\", \"fast\"): [\"cool\", \"warm\"],\n",
        "            (\"warm\", \"slow\"): [\"cool\", \"warm\"],\n",
        "            (\"warm\", \"fast\"): [\"overheated\"]\n",
        "        }\n",
        "        return successors.get((state, action), [])\n",
        "\n",
        "    def possible_actions(self, state):\n",
        "        return [] if state == \"overheated\" else [\"slow\", \"fast\"]\n",
        "\n",
        "\n",
        "class SimpleLeftRightMDP:\n",
        "    state_set = [0, 1, 2, 3, 4, \"exited\"]\n",
        "\n",
        "    def reward(self, state, action, state_prime):\n",
        "        if state == 0 and action == \"exit\":\n",
        "            return 10\n",
        "        elif state == 4 and action == \"exit\":\n",
        "            return 1\n",
        "        else:\n",
        "            return 0\n",
        "\n",
        "    def transition_prob(self, state, action, state_prime):\n",
        "        if state > 0 and action == \"left\" and state_prime == state - 1:\n",
        "            return 1\n",
        "        elif state < 4 and action == \"right\" and state_prime == state + 1:\n",
        "            return 1\n",
        "        elif state in [0, 4] and action == \"exit\" and state_prime == \"exited\":\n",
        "            return 1\n",
        "        else:\n",
        "            return 0\n",
        "\n",
        "    def successor_states(self, state, action):\n",
        "        if state > 0 and action == \"left\":\n",
        "            return [state - 1]\n",
        "        elif state < 4 and action == \"right\":\n",
        "            return [state + 1]\n",
        "        elif state in [0, 4] and action == \"exit\":\n",
        "            return [\"exited\"]\n",
        "        else:\n",
        "            return []\n",
        "\n",
        "    def possible_actions(self, state):\n",
        "        if state in [0, 4]:\n",
        "            return [\"exit\"]\n",
        "        if state == \"exited\":\n",
        "            return []\n",
        "        actions = []\n",
        "        if state < 4:\n",
        "            actions.append(\"right\")\n",
        "        if state > 0:\n",
        "            actions.append(\"left\")\n",
        "        return actions\n",
        "\n",
        "\n",
        "class DoubleBanditsMDP:\n",
        "    state_set = [\"won\", \"lost\"]\n",
        "\n",
        "    def reward(self, state, action, state_prime):\n",
        "        if action == \"red\" and state_prime == \"won\":\n",
        "            return 2\n",
        "        elif action == \"red\" and state_prime == \"lost\":\n",
        "            return 0\n",
        "        elif action == \"blue\":\n",
        "            return 1\n",
        "        else:\n",
        "            return 0\n",
        "\n",
        "    def transition_prob(self, state, action, state_prime):\n",
        "        if action == \"red\" and state_prime == \"won\":\n",
        "            return 0.75\n",
        "        elif action == \"red\" and state_prime == \"lost\":\n",
        "            return 0.25\n",
        "        elif action == \"blue\" and state_prime == \"won\":\n",
        "            return 1\n",
        "        else:\n",
        "            return 0\n",
        "\n",
        "    def successor_states(self, state, action):\n",
        "        if action == \"red\":\n",
        "            return [\"won\", \"lost\"]\n",
        "        elif action == \"blue\":\n",
        "            return [\"won\"]\n",
        "        else:\n",
        "            return []\n",
        "\n",
        "    def possible_actions(self, state):\n",
        "        return [\"red\", \"blue\"]\n",
        "\n",
        "\n",
        "def find_value_function(mdp, num_iterations):\n",
        "    state_set = mdp.state_set\n",
        "    value_function = {state: 0 for state in state_set}\n",
        "\n",
        "    for i in range(num_iterations):\n",
        "        new_value_function = {}\n",
        "        for state in state_set:\n",
        "            action_values = []\n",
        "            for action in mdp.possible_actions(state):\n",
        "                value = sum([\n",
        "                    mdp.transition_prob(state, action, state_prime) *\n",
        "                    (mdp.reward(state, action, state_prime) +\n",
        "                    (mdp.discount_factor * value_function[state_prime]))\n",
        "                    for state_prime in mdp.successor_states(state, action)\n",
        "                ])\n",
        "                action_values.append(value)\n",
        "            new_value_function[state] = max(action_values or [0])\n",
        "        value_function = new_value_function\n",
        "    return value_function\n",
        "\n",
        "def extract_policy(mdp, value_function):\n",
        "    policy = {}\n",
        "    for state in mdp.state_set:\n",
        "        scored_actions = []\n",
        "        for action in mdp.possible_actions(state):\n",
        "            action_value = sum([\n",
        "                mdp.transition_prob(state, action, state_prime) *\n",
        "                (mdp.reward(state, action, state_prime) +\n",
        "                (mdp.discount_factor * value_function[state_prime]))\n",
        "                for state_prime in mdp.successor_states(state, action)\n",
        "            ])\n",
        "            scored_actions.append((action, action_value))\n",
        "        policy[state] = max(scored_actions or [(\"do_nothing\", 0)], key=lambda entry: entry[1])[0]\n",
        "    return policy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " solving a Markov Decision Process (MDP) using Value Iteration"
      ],
      "metadata": {
        "id": "PonaO8gvJ4Rg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mdp = SimpleLeftRightMDP() # change this line to change which MDP you would like to solve\n",
        "mdp.discount_factor = 0.9 # change this line to change the discount factor (gamma)\n",
        "num_iterations = 10 # change this line to specify how many iterations of the Bellman update you would like to perform\n",
        "value_function = find_value_function(mdp, num_iterations)\n",
        "policy = extract_policy(mdp, value_function)\n",
        "print(\"MDP    :\", mdp.__class__.__name__)\n",
        "print(\"VALUE  :\", value_function)\n",
        "print(\"POLICY :\", policy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ucjCXfxJCmq",
        "outputId": "dba9e117-103b-417e-ec71-55aab61c4b26"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MDP    : SimpleLeftRightMDP\n",
            "VALUE  : {0: 10.0, 1: 9.0, 2: 8.1, 3: 7.29, 4: 1.0, 'exited': 0}\n",
            "POLICY : {0: 'exit', 1: 'left', 2: 'left', 3: 'left', 4: 'exit', 'exited': 'do_nothing'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fr3WvF9hJ87I"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}